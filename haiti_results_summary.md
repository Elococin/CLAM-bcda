# Haiti Held-Out CLAM Training Results

## Experiment Overview
- **Dataset**: Haiti held-out (LODO - Leave One Dataset Out)
- **Model**: CLAM-SB with big architecture
- **Training Duration**: 24h 39m (51 epochs, early stopped)
- **GPU Utilization**: Optimized from 2% to 42% peaks

## Final Performance Metrics
- **Validation AUC**: 87.43%
- **Test AUC**: 78.52%
- **Validation Error**: 15.24%
- **Test Error**: 26.85%
- **Class 0 Test Accuracy**: 50.27%
- **Class 1 Test Accuracy**: 88.06%

## Training Configuration
- **Model Size**: big
- **Batch Parameter (B)**: 64
- **Bag Weight**: 0.7
- **Embed Dimension**: 768 (CONCH)
- **Number of Workers**: 3
- **Learning Rate**: 0.0001
- **Dropout**: 0.25

## Technical Achievements
- **DataLoader Optimization**: Fixed worker configuration bug
- **GPU Utilization**: 21x improvement (2% â†’ 42% peaks)
- **Resource Efficiency**: 3 CPU workers + 1 main process
- **Training Speed**: ~2.8 seconds per batch average

## Key Files Generated
- `s_0_checkpoint.pt` - Trained model weights (3.2MB)
- `split_0_results.pkl` - Detailed results and metrics (962KB)
- `experiment_default.txt` - Experiment configuration
- Job log: `clam_er_lodo_optimized.sbatch-7255941.out`

## Early Stopping Details
- **Patience**: 20 epochs
- **Triggered**: After 41 consecutive epochs without validation improvement
- **Best Epoch**: ~10-20 (based on validation plateau)

## Class Performance Analysis
- **Class Imbalance**: Evident from performance difference
- **Class 1 (Positive)**: Strong performance (88.06% accuracy)
- **Class 0 (Negative)**: Weaker performance (50.27% accuracy)
- **Overall**: Good AUC indicates model distinguishes classes well despite imbalance

## Technical Notes
- Training completed successfully within 48h time limit
- Job marked as "FAILED" due to missing splits_1.csv (expected for single-split run)
- All core results and model artifacts saved successfully
- GPU monitoring logs available for performance analysis

Date: October 10-11, 2025
Server: MLSC (rtx-08 node)
